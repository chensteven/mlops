{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics.functional import accuracy, auroc, f1_score\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (AdamW, AutoModel, AutoTokenizer, BertConfig,\n",
    "                          BertModel, BertTokenizer)\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/post_categories/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Four Steps To Turn Big Data Into Action</td>\n",
       "      <td>['big data']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9 Amazing Ways Big Data Is Used Today to Chang...</td>\n",
       "      <td>['big data']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science and Moneyball: A Profile of Pete ...</td>\n",
       "      <td>['data science']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stanford Algorithm Analyzes Sentence Sentiment...</td>\n",
       "      <td>['machine learning']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Machine Learning for Relevance and Serendipity</td>\n",
       "      <td>['machine learning']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>Word2Vec Skip The Gram Tutorial</td>\n",
       "      <td>['nlp', 'machine learning']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>150 successful machine learning models: 6 less...</td>\n",
       "      <td>['machine learning', 'application']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>Introduction to Adversarial Machine Learning</td>\n",
       "      <td>['machine learning']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>The Craft of Writing Effectively</td>\n",
       "      <td>['career']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>Use Dalle 2 to generate logo</td>\n",
       "      <td>['deep learning', 'application']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>333 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0              Four Steps To Turn Big Data Into Action   \n",
       "1    9 Amazing Ways Big Data Is Used Today to Chang...   \n",
       "2    Data Science and Moneyball: A Profile of Pete ...   \n",
       "3    Stanford Algorithm Analyzes Sentence Sentiment...   \n",
       "4       Machine Learning for Relevance and Serendipity   \n",
       "..                                                 ...   \n",
       "328                    Word2Vec Skip The Gram Tutorial   \n",
       "329  150 successful machine learning models: 6 less...   \n",
       "330       Introduction to Adversarial Machine Learning   \n",
       "331                   The Craft of Writing Effectively   \n",
       "332                       Use Dalle 2 to generate logo   \n",
       "\n",
       "                                   label  \n",
       "0                           ['big data']  \n",
       "1                           ['big data']  \n",
       "2                       ['data science']  \n",
       "3                   ['machine learning']  \n",
       "4                   ['machine learning']  \n",
       "..                                   ...  \n",
       "328          ['nlp', 'machine learning']  \n",
       "329  ['machine learning', 'application']  \n",
       "330                 ['machine learning']  \n",
       "331                           ['career']  \n",
       "332     ['deep learning', 'application']  \n",
       "\n",
       "[333 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'big data'}, {'big data'}, {'data science'}, {'machine learning'}, {'machine learning'}, {'python', 'data science'}, {'data visualization'}, {'computer vision'}, {'data'}, {'interview'}, {'use cases', 'big data'}, {'machine learning'}, {'healthcare', 'big data'}, {'engineering'}, {'data science'}, {'big data'}, {'interview'}, {'data science'}, {'forecasting'}, {'neural network', 'deep learning'}, {'use case'}, {'guide'}, {'nlp'}, {'industry'}, {'nlp'}, {'nlp'}, {'guide'}, {'python', 'designed patterns'}, {'interview'}, {'reinforcement learning'}, {'review', 'data science'}, {'question', 'machine learning'}, {'algorithm'}, {'algorithm'}, {'deep learning'}, {'deep learning'}, {'data science'}, {'question', 'data science'}, {'libraries'}, {'deep learning'}, {'trading'}, {'computer vision'}, {'applications', 'machine learning'}, {'how-to', 'data science'}, {'data shift', 'mlops'}, {'interview', 'business', 'data science'}, {'deep learning'}, {'computer science'}, {'healthcare', 'data science'}, {'computer vision', 'machine learning'}, {'gis'}, {'python', 'data science'}, {'algorithm'}, {'data visualization'}, {'computer vision'}, {'application', 'machine learning'}, {'ai'}, {'course', 'statistics'}, {'resource', 'data science'}, {'libraries'}, {'libraries'}, {'data mining'}, {'deep learning'}, {'python', 'forecasting'}, {'algorithm'}, {'bandits', 'statistics', 'bayesian'}, {'industry'}, {'bayesian'}, {'papers'}, {'mlops'}, {'use case'}, {'computer vision'}, {'dataset'}, {'tutorials'}, {'statistics', 'python'}, {'use case'}, {'interview'}, {'cloud'}, {'ai', 'deep learning'}, {'ai'}, {'python', 'deep learning'}, {'reinforcement learning'}, {'deep learning'}, {'ai'}, {'python'}, {'recommendation system'}, {'clustering'}, {'statistics'}, {'probability'}, {'probability', 'machine learning'}, {'robotics', 'ai'}, {'iot', 'deep learning'}, {'interview'}, {'applications', 'machine learning'}, {'courses'}, {'books'}, {'tools', 'python'}, {'data science'}, {'machine learning'}, {'forecasting'}, {'big data'}, {'recommendation system'}, {'bandits', 'bayesian'}, {'data visualization', 'algorithm'}, {'courses'}, {'libraries'}, {'predictive analytics'}, {'how-to'}, {'libraries'}, {'interviews'}, {'real-time'}, {'deep learning'}, {'nlp'}, {'gis'}, {'neural network'}, {'interview'}, {'salary'}, {'library', 'business'}, {'computer vision', 'deep learning'}, {'libraries'}, {'data science'}, {'recommendation system'}, {'statistics'}, {'neural network'}, {'neural network', 'deep learning'}, {'neural network'}, {'bandits', 'python'}, {'use case'}, {'use case'}, {'recommendation system'}, {'bayesian'}, {'algorithm'}, {'experimentation'}, {'reinforcement learning'}, {'hardware'}, {'mathematics'}, {'machine learning'}, {'bayesian'}, {'machine learning'}, {'algorithm'}, {'statistics', 'decision making'}, {'neural network', 'graph'}, {'nlp', 'deep learning'}, {'application', 'machine learning'}, {'neural network', 'deep learning'}, {'bayesian', 'machine learning'}, {'mlops', 'machine learning'}, {'deep learning'}, {'course'}, {'mlops'}, {'reinforcement learning'}, {'sql'}, {'improvement'}, {'application', 'data science'}, {'career'}, {'statistics', 'data science'}, {'algorithms', 'machine learning'}, {'nlp', 'machine learning'}, {'best practices'}, {'mlops'}, {'research'}, {'research'}, {'data engineering'}, {'mathematics'}, {'nlp'}, {'probability'}, {'infrastructure'}, {'mlops'}, {'simulation'}, {'decision making'}, {'decision making'}, {'nlp'}, {'career'}, {'hardware'}, {'data visualization'}, {'libraries'}, {'graph'}, {'recommendation system'}, {'tutorial'}, {'sql'}, {'mlops'}, {'mlops'}, {'neural network'}, {'probability'}, {'system design', 'machine learning'}, {'nlp'}, {'product'}, {'dashboard'}, {'bayesian'}, {'algorithm'}, {'nlp'}, {'algorithm'}, {'graph'}, {'application'}, {'computer vision'}, {'deep learning'}, {'hardware'}, {'data visualization'}, {'design patterns'}, {'recommendation system'}, {'books'}, {'books'}, {'mathematics'}, {'theory'}, {'papers'}, {'course', 'deep learning'}, {'resource'}, {'notes', 'mathematics'}, {'how-to'}, {'python', 'recommendation system'}, {'ux'}, {'web development'}, {'application', 'data science'}, {'mobile development'}, {'guide', 'sql'}, {'mobile development', 'tutorial'}, {'visualization', 'mathematics'}, {'web development'}, {'career'}, {'ux'}, {'interview'}, {'hiring'}, {'python', 'recommendation system'}, {'how-to', 'mathematics'}, {'nlp'}, {'web development'}, {'algorithms', 'machine learning'}, {'python', 'algorithms'}, {'guide'}, {'computer science', 'course'}, {'statistics', 'visualization'}, {'mathematics'}, {'deep learning'}, {'mathematics'}, {'machine learning', 'mathematics'}, {'papers'}, {'machine learning'}, {'neural network', 'deep learning'}, {'computer science'}, {'deep learning'}, {'web development'}, {'best practice', 'python'}, {'computer science'}, {'computer science'}, {'mathematics'}, {'algorithm'}, {'mathematics'}, {'machine learning'}, {'mathematics', 'machine learning'}, {'software'}, {'deep learning'}, {'computer vision'}, {'nlp', 'machine learning'}, {'machine learning'}, {'data visualization'}, {'tutorial', 'machine learning'}, {'mathematics'}, {'computer vision', 'neural network'}, {'mathematics'}, {'course', 'deep learning'}, {'books', 'mathematics'}, {'course', 'mathematics'}, {'guide', 'deep learning'}, {'computer science'}, {'computer science'}, {'algorithm'}, {'computer science', 'python'}, {'computer science', 'python'}, {'sql'}, {'python'}, {'neural network', 'deep learning'}, {'mathematics', 'ai', 'machine learning'}, {'application', 'machine learning'}, {'application'}, {'machine learning', 'visualization', 'algorithm', 'data science'}, {'neural network', 'deep learning'}, {'book', 'machine learning'}, {'programming'}, {'machine learning'}, {'neural network', 'deep learning'}, {'computer science', 'mlops'}, {'how-to', 'career'}, {'mlops'}, {'machine learning'}, {'ai', 'book'}, {'data visualization', 'statistics', 'data science'}, {'guide', 'career'}, {'algorithms', 'machine learning'}, {'computer science', 'research'}, {'algorithm'}, {'neural network', 'deep learning'}, {'books', 'mathematics'}, {'deep learning'}, {'reinforcement learning'}, {'how-to', 'machine learning'}, {'computer science', 'rant'}, {'papers'}, {'web development'}, {'career'}, {'computer science'}, {'algorithms', 'machine learning'}, {'algorithms', 'machine learning'}, {'algorithms', 'machine learning'}, {'ai'}, {'mathematics'}, {'computer science', 'python', 'algorithms'}, {'guide', 'sql'}, {'visualization', 'algorithms'}, {'career'}, {'computer science'}, {'computer science'}, {'guide', 'career'}, {'web development'}, {'mathematics'}, {'deep learning'}, {'mathematics'}, {'computer science'}, {'nlp'}, {'neural network', 'deep learning'}, {'course', 'reinforcement learning'}, {'mathematics'}, {'computer science', 'ai', 'paper', 'machine learning'}, {'mathematics'}, {'course', 'reinforcement learning'}, {'course', 'reinforcement learning'}, {'nlp', 'tutorial', 'machine learning'}, {'course', 'machine learning'}, {'computer vision', 'neural network'}, {'nlp', 'machine learning'}, {'application', 'machine learning'}, {'machine learning'}, {'career'}, {'application', 'deep learning'}]\n"
     ]
    }
   ],
   "source": [
    "vocabs = []\n",
    "for d in df['label']:\n",
    "  x_list = ast.literal_eval(d)\n",
    "  vocabs.append(set(x_list))\n",
    "print(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "preds = mlb.fit_transform(vocabs)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...\n",
      "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...\n",
      "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "                             ...                        \n",
      "328    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "329    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "330    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "331    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...\n",
      "332    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "Name: label, Length: 333, dtype: object\n"
     ]
    }
   ],
   "source": [
    "y = df['label'].apply(ast.literal_eval)\n",
    "y = y.apply(lambda x: [set(x)])\n",
    "y = y.apply(mlb.transform)\n",
    "y = y.apply(lambda x: x[0])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['title']\n",
    "y = y\n",
    "df_cpy = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ai', 'algorithm', 'algorithms', 'application', 'applications',\n",
      "       'bandits', 'bayesian', 'best practice', 'best practices', 'big data',\n",
      "       'book', 'books', 'business', 'career', 'cloud', 'clustering',\n",
      "       'computer science', 'computer vision', 'course', 'courses', 'dashboard',\n",
      "       'data', 'data engineering', 'data mining', 'data science', 'data shift',\n",
      "       'data visualization', 'dataset', 'decision making', 'deep learning',\n",
      "       'design patterns', 'designed patterns', 'engineering',\n",
      "       'experimentation', 'forecasting', 'gis', 'graph', 'guide', 'hardware',\n",
      "       'healthcare', 'hiring', 'how-to', 'improvement', 'industry',\n",
      "       'infrastructure', 'interview', 'interviews', 'iot', 'libraries',\n",
      "       'library', 'machine learning', 'mathematics', 'mlops',\n",
      "       'mobile development', 'neural network', 'nlp', 'notes', 'paper',\n",
      "       'papers', 'predictive analytics', 'probability', 'product',\n",
      "       'programming', 'python', 'question', 'rant', 'real-time',\n",
      "       'recommendation system', 'reinforcement learning', 'research',\n",
      "       'resource', 'review', 'robotics', 'salary', 'simulation', 'software',\n",
      "       'sql', 'statistics', 'system design', 'theory', 'tools', 'trading',\n",
      "       'tutorial', 'tutorials', 'use case', 'use cases', 'ux', 'visualization',\n",
      "       'web development'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "dummies = df_cpy['label'].apply(lambda x: eval(x)).str.join('|').str.get_dummies()\n",
    "LABEL_COLUMNS = dummies.columns\n",
    "print(LABEL_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ai  algorithm  algorithms  application  applications  bandits  bayesian  \\\n",
      "0     0          0           0            0             0        0         0   \n",
      "1     0          0           0            0             0        0         0   \n",
      "2     0          0           0            0             0        0         0   \n",
      "3     0          0           0            0             0        0         0   \n",
      "4     0          0           0            0             0        0         0   \n",
      "..   ..        ...         ...          ...           ...      ...       ...   \n",
      "328   0          0           0            0             0        0         0   \n",
      "329   0          0           0            1             0        0         0   \n",
      "330   0          0           0            0             0        0         0   \n",
      "331   0          0           0            0             0        0         0   \n",
      "332   0          0           0            1             0        0         0   \n",
      "\n",
      "     best practice  best practices  big data  ...  theory  tools  trading  \\\n",
      "0                0               0         1  ...       0      0        0   \n",
      "1                0               0         1  ...       0      0        0   \n",
      "2                0               0         0  ...       0      0        0   \n",
      "3                0               0         0  ...       0      0        0   \n",
      "4                0               0         0  ...       0      0        0   \n",
      "..             ...             ...       ...  ...     ...    ...      ...   \n",
      "328              0               0         0  ...       0      0        0   \n",
      "329              0               0         0  ...       0      0        0   \n",
      "330              0               0         0  ...       0      0        0   \n",
      "331              0               0         0  ...       0      0        0   \n",
      "332              0               0         0  ...       0      0        0   \n",
      "\n",
      "     tutorial  tutorials  use case  use cases  ux  visualization  \\\n",
      "0           0          0         0          0   0              0   \n",
      "1           0          0         0          0   0              0   \n",
      "2           0          0         0          0   0              0   \n",
      "3           0          0         0          0   0              0   \n",
      "4           0          0         0          0   0              0   \n",
      "..        ...        ...       ...        ...  ..            ...   \n",
      "328         0          0         0          0   0              0   \n",
      "329         0          0         0          0   0              0   \n",
      "330         0          0         0          0   0              0   \n",
      "331         0          0         0          0   0              0   \n",
      "332         0          0         0          0   0              0   \n",
      "\n",
      "     web development  \n",
      "0                  0  \n",
      "1                  0  \n",
      "2                  0  \n",
      "3                  0  \n",
      "4                  0  \n",
      "..               ...  \n",
      "328                0  \n",
      "329                0  \n",
      "330                0  \n",
      "331                0  \n",
      "332                0  \n",
      "\n",
      "[333 rows x 89 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 title  \\\n",
      "0              Four Steps To Turn Big Data Into Action   \n",
      "1    9 Amazing Ways Big Data Is Used Today to Chang...   \n",
      "2    Data Science and Moneyball: A Profile of Pete ...   \n",
      "3    Stanford Algorithm Analyzes Sentence Sentiment...   \n",
      "4       Machine Learning for Relevance and Serendipity   \n",
      "..                                                 ...   \n",
      "328                    Word2Vec Skip The Gram Tutorial   \n",
      "329  150 successful machine learning models: 6 less...   \n",
      "330       Introduction to Adversarial Machine Learning   \n",
      "331                   The Craft of Writing Effectively   \n",
      "332                       Use Dalle 2 to generate logo   \n",
      "\n",
      "                                   label  ai  algorithm  algorithms  \\\n",
      "0                           ['big data']   0          0           0   \n",
      "1                           ['big data']   0          0           0   \n",
      "2                       ['data science']   0          0           0   \n",
      "3                   ['machine learning']   0          0           0   \n",
      "4                   ['machine learning']   0          0           0   \n",
      "..                                   ...  ..        ...         ...   \n",
      "328          ['nlp', 'machine learning']   0          0           0   \n",
      "329  ['machine learning', 'application']   0          0           0   \n",
      "330                 ['machine learning']   0          0           0   \n",
      "331                           ['career']   0          0           0   \n",
      "332     ['deep learning', 'application']   0          0           0   \n",
      "\n",
      "     application  applications  bandits  bayesian  best practice  ...  theory  \\\n",
      "0              0             0        0         0              0  ...       0   \n",
      "1              0             0        0         0              0  ...       0   \n",
      "2              0             0        0         0              0  ...       0   \n",
      "3              0             0        0         0              0  ...       0   \n",
      "4              0             0        0         0              0  ...       0   \n",
      "..           ...           ...      ...       ...            ...  ...     ...   \n",
      "328            0             0        0         0              0  ...       0   \n",
      "329            1             0        0         0              0  ...       0   \n",
      "330            0             0        0         0              0  ...       0   \n",
      "331            0             0        0         0              0  ...       0   \n",
      "332            1             0        0         0              0  ...       0   \n",
      "\n",
      "     tools  trading  tutorial  tutorials  use case  use cases  ux  \\\n",
      "0        0        0         0          0         0          0   0   \n",
      "1        0        0         0          0         0          0   0   \n",
      "2        0        0         0          0         0          0   0   \n",
      "3        0        0         0          0         0          0   0   \n",
      "4        0        0         0          0         0          0   0   \n",
      "..     ...      ...       ...        ...       ...        ...  ..   \n",
      "328      0        0         0          0         0          0   0   \n",
      "329      0        0         0          0         0          0   0   \n",
      "330      0        0         0          0         0          0   0   \n",
      "331      0        0         0          0         0          0   0   \n",
      "332      0        0         0          0         0          0   0   \n",
      "\n",
      "     visualization  web development  \n",
      "0                0                0  \n",
      "1                0                0  \n",
      "2                0                0  \n",
      "3                0                0  \n",
      "4                0                0  \n",
      "..             ...              ...  \n",
      "328              0                0  \n",
      "329              0                0  \n",
      "330              0                0  \n",
      "331              0                0  \n",
      "332              0                0  \n",
      "\n",
      "[329 rows x 91 columns]\n"
     ]
    }
   ],
   "source": [
    "df_final = pd.merge(df_cpy, dummies, left_index=True, right_index=True).dropna()\n",
    "print(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(184, 91) (99, 91) (46, 91)\n"
     ]
    }
   ],
   "source": [
    "# First Split for Train and Test\n",
    "train, test = train_test_split(df_final, random_state=42, test_size=0.30, shuffle=True)\n",
    "train, val = train_test_split(train, test_size=0.2, random_state=42,shuffle=True)\n",
    "print(train.shape, test.shape, val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ai                 5\n",
       "algorithm          9\n",
       "algorithms         1\n",
       "application        5\n",
       "applications       0\n",
       "                  ..\n",
       "use case           3\n",
       "use cases          1\n",
       "ux                 2\n",
       "visualization      2\n",
       "web development    6\n",
       "Length: 89, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[LABEL_COLUMNS].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Amazing Image Algorithm Learns to Spot Objects Without Human Help\n",
      "\n",
      "{'ai': 0, 'algorithm': 0, 'algorithms': 0, 'application': 0, 'applications': 0, 'bandits': 0, 'bayesian': 0, 'best practice': 0, 'best practices': 0, 'big data': 0, 'book': 0, 'books': 0, 'business': 0, 'career': 0, 'cloud': 0, 'clustering': 0, 'computer science': 0, 'computer vision': 1, 'course': 0, 'courses': 0, 'dashboard': 0, 'data': 0, 'data engineering': 0, 'data mining': 0, 'data science': 0, 'data shift': 0, 'data visualization': 0, 'dataset': 0, 'decision making': 0, 'deep learning': 0, 'design patterns': 0, 'designed patterns': 0, 'engineering': 0, 'experimentation': 0, 'forecasting': 0, 'gis': 0, 'graph': 0, 'guide': 0, 'hardware': 0, 'healthcare': 0, 'hiring': 0, 'how-to': 0, 'improvement': 0, 'industry': 0, 'infrastructure': 0, 'interview': 0, 'interviews': 0, 'iot': 0, 'libraries': 0, 'library': 0, 'machine learning': 0, 'mathematics': 0, 'mlops': 0, 'mobile development': 0, 'neural network': 0, 'nlp': 0, 'notes': 0, 'paper': 0, 'papers': 0, 'predictive analytics': 0, 'probability': 0, 'product': 0, 'programming': 0, 'python': 0, 'question': 0, 'rant': 0, 'real-time': 0, 'recommendation system': 0, 'reinforcement learning': 0, 'research': 0, 'resource': 0, 'review': 0, 'robotics': 0, 'salary': 0, 'simulation': 0, 'software': 0, 'sql': 0, 'statistics': 0, 'system design': 0, 'theory': 0, 'tools': 0, 'trading': 0, 'tutorial': 0, 'tutorials': 0, 'use case': 0, 'use cases': 0, 'ux': 0, 'visualization': 0, 'web development': 0}\n"
     ]
    }
   ],
   "source": [
    "sample_row = train.iloc[16]\n",
    "sample_comment = sample_row.title\n",
    "sample_labels = sample_row[LABEL_COLUMNS]\n",
    "\n",
    "print(sample_comment)\n",
    "print()\n",
    "print(sample_labels.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading tokenizer of bert base version\n",
    "BERT_MODEL_NAME = \"bert-base-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "encoding = tokenizer.encode_plus(\n",
    "    sample_comment, \n",
    "    add_special_tokens=True,\n",
    "    max_length=512,\n",
    "    return_token_type_ids=False,\n",
    "    padding='max_length',\n",
    "    return_attention_mask=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512]) torch.Size([1, 512])\n",
      "tensor([  101,  1188, 16035, 15065,  2586, 18791,  7088,  1306, 12958, 20163,\n",
      "         1106, 28091,   152, 24380,  1116,  4914,  4243, 12056,   102,     0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0])\n",
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "print(encoding[\"input_ids\"].shape, encoding[\"attention_mask\"].shape)\n",
    "print(encoding[\"input_ids\"].squeeze()[:20])\n",
    "print(encoding[\"attention_mask\"].squeeze()[:20])\n",
    "print(encoding[\"input_ids\"].squeeze().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'This',\n",
       " 'Amazing',\n",
       " 'Image',\n",
       " 'Al',\n",
       " '##gor',\n",
       " '##ith',\n",
       " '##m',\n",
       " 'Lea',\n",
       " '##rns',\n",
       " 'to',\n",
       " 'Spot',\n",
       " 'O',\n",
       " '##bject',\n",
       " '##s',\n",
       " 'Without',\n",
       " 'Human',\n",
       " 'Help',\n",
       " '[SEP]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"].squeeze())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../model/tokenizer/tokenizer_config.json',\n",
       " '../model/tokenizer/special_tokens_map.json',\n",
       " '../model/tokenizer/vocab.txt',\n",
       " '../model/tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('../model/tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostTitleDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame, tokenizer: BertTokenizer, max_token_len: int = 128):\n",
    "\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        data_row = self.data.iloc[index]\n",
    "\n",
    "        title = data_row.title\n",
    "        labels = data_row[LABEL_COLUMNS]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_token_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return dict(\n",
    "            title=title,\n",
    "            input_ids=encoding[\"input_ids\"].flatten(),\n",
    "            attention_mask=encoding[\"attention_mask\"].flatten(),\n",
    "            labels=torch.FloatTensor(labels)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PostTitleDataset(train, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['title', 'input_ids', 'attention_mask', 'labels'])\n",
      "Algorithmic Trading Models - Machine Learning\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "sample_item = train_dataset[0]\n",
    "print(sample_item.keys())\n",
    "print(sample_item[\"title\"])\n",
    "print(sample_item[\"labels\"])\n",
    "print(sample_item[\"input_ids\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained(BERT_MODEL_NAME, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128, 768]), torch.Size([1, 768]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_item[\"input_ids\"].unsqueeze(dim=0).shape\n",
    "prediction = bert_model(sample_item[\"input_ids\"].unsqueeze(dim=0), sample_item[\"attention_mask\"].unsqueeze(dim=0))\n",
    "prediction.last_hidden_state.shape, prediction.pooler_output.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostTitleDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_df, test_df, tokenizer, batch_size=0, max_token_len=128):\n",
    "        super().__init__\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.max_token_len = max_token_len\n",
    "        self.prepare_data_per_node = True\n",
    "        self._log_hyperparams = True\n",
    "\n",
    "    def setup(self, stage):\n",
    "        self.train_dataset = PostTitleDataset(\n",
    "            self.train_df,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len\n",
    "        )\n",
    "\n",
    "        self.test_dataset = PostTitleDataset(\n",
    "            self.test_df,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=1, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=1, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "data_module = PostTitleDataModule(train, test, tokenizer, batch_size=BATCH_SIZE)\n",
    "data_module.setup(stage=\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 0.7452, 0.8299, 0.5096, 1.0000, 0.7493])\n",
      "tensor(0.8725)\n"
     ]
    }
   ],
   "source": [
    "# demo what BCE is\n",
    "criterion = nn.BCELoss()\n",
    "prediction = torch.FloatTensor(\n",
    "    [10.95873564, 1.07321467, 1.58524066, 0.03839076, 15.72987556, 1.09513213]\n",
    ")\n",
    "labels = torch.FloatTensor(\n",
    "  [1., 0., 0., 0., 1., 0.]\n",
    ")\n",
    "print(torch.sigmoid(prediction))\n",
    "output = criterion(torch.sigmoid(prediction), labels)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostTitleCategoryModel(pl.LightningModule):\n",
    "\n",
    "  def __init__(self, n_classes: int, n_training_steps=None, n_warmup_steps=None):\n",
    "    super().__init__()\n",
    "    self.bert = BertModel.from_pretrained(BERT_MODEL_NAME, return_dict=True)\n",
    "    self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "    self.n_training_steps = n_training_steps\n",
    "    self.n_warmup_steps = n_warmup_steps\n",
    "    self.criterion = nn.BCELoss()\n",
    "\n",
    "  def forward(self, input_ids, attention_mask, labels=None):\n",
    "    output = self.bert(input_ids, attention_mask=attention_mask)\n",
    "    output = self.classifier(output.pooler_output)\n",
    "    output = torch.sigmoid(output)    \n",
    "    loss = 0\n",
    "    if labels is not None:\n",
    "        loss = self.criterion(output, labels)\n",
    "    return loss, output\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    loss, outputs = self(input_ids, attention_mask, labels)\n",
    "    self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "    return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    loss, outputs = self(input_ids, attention_mask, labels)\n",
    "    self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "    return loss\n",
    "\n",
    "  def test_step(self, batch, batch_idx):\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    loss, outputs = self(input_ids, attention_mask, labels)\n",
    "    self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "    return loss\n",
    "\n",
    "  def training_epoch_end(self, outputs):\n",
    "    \n",
    "    labels = []\n",
    "    predictions = []\n",
    "    for output in outputs:\n",
    "      for out_labels in output[\"labels\"].detach().cpu():\n",
    "        labels.append(out_labels)\n",
    "      for out_predictions in output[\"predictions\"].detach().cpu():\n",
    "        predictions.append(out_predictions)\n",
    "\n",
    "    labels = torch.stack(labels).int()\n",
    "    predictions = torch.stack(predictions)\n",
    "\n",
    "    for i, name in enumerate(LABEL_COLUMNS):\n",
    "      class_roc_auc = auroc(predictions[:, i], labels[:, i])\n",
    "      self.logger.experiment.add_scalar(f\"{name}_roc_auc/Train\", class_roc_auc, self.current_epoch)\n",
    "\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "\n",
    "    optimizer = AdamW(self.parameters(), lr=2e-5)\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "      optimizer,\n",
    "      num_warmup_steps=self.n_warmup_steps,\n",
    "      num_training_steps=self.n_training_steps\n",
    "    )\n",
    "\n",
    "    return dict(\n",
    "      optimizer=optimizer,\n",
    "      lr_scheduler=dict(\n",
    "        scheduler=scheduler,\n",
    "        interval='step'\n",
    "      )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 50)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch=len(train) // BATCH_SIZE\n",
    "total_training_steps = steps_per_epoch * N_EPOCHS\n",
    "warmup_steps = total_training_steps // 5\n",
    "warmup_steps, total_training_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = PostTitleCategoryModel(\n",
    "  n_classes=len(LABEL_COLUMNS),\n",
    "  n_warmup_steps=warmup_steps,\n",
    "  n_training_steps=total_training_steps \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.bert.save_pretrained('../model/bert_pretrained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "  dirpath=\"checkpoints\",\n",
    "  filename=\"best-checkpoint\",\n",
    "  save_top_k=1,\n",
    "  verbose=True,\n",
    "  monitor=\"val_loss\",\n",
    "  mode=\"min\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "  logger=True,\n",
    "  callbacks=[early_stopping_callback, checkpoint_callback],\n",
    "  max_epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /Users/schen/wecloud/mlops_labs_term2/training/lightning_logs\n",
      "/Users/schen/.pyenv/versions/3.7.8/envs/wecloud378/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "\n",
      "  | Name       | Type      | Params\n",
      "-----------------------------------------\n",
      "0 | bert       | BertModel | 108 M \n",
      "1 | classifier | Linear    | 68.4 K\n",
      "2 | criterion  | BCELoss   | 0     \n",
      "-----------------------------------------\n",
      "108 M     Trainable params\n",
      "0         Non-trainable params\n",
      "108 M     Total params\n",
      "433.515   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/schen/.pyenv/versions/3.7.8/envs/wecloud378/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:1898: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  category=PossibleUserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [03:19<00:00,  1.90s/it, loss=0.717, v_num=0, train_loss=0.704, val_loss=0.693]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/schen/.pyenv/versions/3.7.8/envs/wecloud378/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [03:19<00:00,  1.90s/it, loss=0.717, v_num=0, train_loss=0.704, val_loss=0.693]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 6: 'val_loss' reached 0.69255 (best 0.69255), saving model to '/Users/schen/wecloud/mlops_labs_term2/training/checkpoints/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [02:44<00:00,  1.57s/it, loss=0.694, v_num=0, train_loss=0.655, val_loss=0.652]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 12: 'val_loss' reached 0.65192 (best 0.65192), saving model to '/Users/schen/wecloud/mlops_labs_term2/training/checkpoints/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [02:11<00:00,  1.26s/it, loss=0.675, v_num=0, train_loss=0.618, val_loss=0.611]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 18: 'val_loss' reached 0.61060 (best 0.61060), saving model to '/Users/schen/wecloud/mlops_labs_term2/training/checkpoints/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [01:59<00:00,  1.14s/it, loss=0.643, v_num=0, train_loss=0.587, val_loss=0.575]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 24: 'val_loss' reached 0.57528 (best 0.57528), saving model to '/Users/schen/wecloud/mlops_labs_term2/training/checkpoints/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [02:54<00:00,  1.66s/it, loss=0.606, v_num=0, train_loss=0.555, val_loss=0.545]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 30: 'val_loss' reached 0.54469 (best 0.54469), saving model to '/Users/schen/wecloud/mlops_labs_term2/training/checkpoints/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [02:56<00:00,  1.68s/it, loss=0.573, v_num=0, train_loss=0.529, val_loss=0.518]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 36: 'val_loss' reached 0.51811 (best 0.51811), saving model to '/Users/schen/wecloud/mlops_labs_term2/training/checkpoints/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [02:34<00:00,  1.47s/it, loss=0.545, v_num=0, train_loss=0.509, val_loss=0.504]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 42: 'val_loss' reached 0.50354 (best 0.50354), saving model to '/Users/schen/wecloud/mlops_labs_term2/training/checkpoints/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [02:34<00:00,  1.47s/it, loss=0.524, v_num=0, train_loss=0.500, val_loss=0.495]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 48: 'val_loss' reached 0.49491 (best 0.49491), saving model to '/Users/schen/wecloud/mlops_labs_term2/training/checkpoints/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [01:51<00:00,  1.06s/it, loss=0.51, v_num=0, train_loss=0.503, val_loss=0.494] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 54: 'val_loss' reached 0.49399 (best 0.49399), saving model to '/Users/schen/wecloud/mlops_labs_term2/training/checkpoints/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [01:44<00:00,  1.00it/s, loss=0.503, v_num=0, train_loss=0.499, val_loss=0.494]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 60: 'val_loss' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [01:44<00:00,  1.00it/s, loss=0.503, v_num=0, train_loss=0.499, val_loss=0.494]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TitleTagger(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=89, bias=True)\n",
       "  (criterion): BCELoss()\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.freeze()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../model/model_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comment = \"Deep Learning course at UofT\"\n",
    "\n",
    "encoding = tokenizer.encode_plus(\n",
    "  test_comment,\n",
    "  add_special_tokens=True,\n",
    "  max_length=512,\n",
    "  return_token_type_ids=False,\n",
    "  padding=\"max_length\",\n",
    "  return_attention_mask=True,\n",
    "  return_tensors='pt',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai: 0.5314832329750061\n",
      "algorithm: 0.3316115736961365\n",
      "algorithms: 0.2727877199649811\n",
      "application: 0.48776936531066895\n",
      "applications: 0.5289465188980103\n",
      "bandits: 0.2312105894088745\n",
      "bayesian: 0.35741063952445984\n",
      "best practice: 0.30665919184684753\n",
      "best practices: 0.18563441932201385\n",
      "big data: 0.449453741312027\n",
      "book: 0.4171124994754791\n",
      "books: 0.4235944151878357\n",
      "business: 0.31198379397392273\n",
      "career: 0.3096870183944702\n",
      "cloud: 0.37051311135292053\n",
      "clustering: 0.5439156293869019\n",
      "computer science: 0.42995527386665344\n",
      "computer vision: 0.5262809991836548\n",
      "course: 0.40591341257095337\n",
      "courses: 0.5615496039390564\n",
      "dashboard: 0.46855491399765015\n",
      "data: 0.5176693797111511\n",
      "data engineering: 0.24762529134750366\n",
      "data mining: 0.47928252816200256\n",
      "data science: 0.36023592948913574\n",
      "data shift: 0.3331622779369354\n",
      "data visualization: 0.4803379774093628\n",
      "dataset: 0.38014107942581177\n",
      "decision making: 0.26210933923721313\n",
      "deep learning: 0.43653497099876404\n",
      "design patterns: 0.3018726706504822\n",
      "designed patterns: 0.4112549424171448\n",
      "engineering: 0.30747097730636597\n",
      "experimentation: 0.372930109500885\n",
      "forecasting: 0.3778208792209625\n",
      "gis: 0.3222866356372833\n",
      "graph: 0.37529587745666504\n",
      "guide: 0.4282357096672058\n",
      "hardware: 0.42728766798973083\n",
      "healthcare: 0.5136735439300537\n",
      "hiring: 0.3517167270183563\n",
      "how-to: 0.447263240814209\n",
      "improvement: 0.34361571073532104\n",
      "industry: 0.274649441242218\n",
      "infrastructure: 0.42002174258232117\n",
      "interview: 0.23578600585460663\n",
      "interviews: 0.42447349429130554\n",
      "iot: 0.49682024121284485\n",
      "libraries: 0.22881492972373962\n",
      "library: 0.5004120469093323\n",
      "machine learning: 0.4167049825191498\n",
      "mathematics: 0.3968923091888428\n",
      "mlops: 0.39180004596710205\n",
      "mobile development: 0.32369425892829895\n",
      "neural network: 0.40963828563690186\n",
      "nlp: 0.5619704127311707\n",
      "notes: 0.3014512360095978\n",
      "paper: 0.46894195675849915\n",
      "papers: 0.3601011335849762\n",
      "predictive analytics: 0.14994846284389496\n",
      "probability: 0.27379509806632996\n",
      "product: 0.34789717197418213\n",
      "programming: 0.4422869086265564\n",
      "python: 0.33905699849128723\n",
      "question: 0.3074190616607666\n",
      "rant: 0.39707151055336\n",
      "real-time: 0.4504814147949219\n",
      "recommendation system: 0.18199656903743744\n",
      "reinforcement learning: 0.305024117231369\n",
      "research: 0.37981218099594116\n",
      "resource: 0.3789472281932831\n",
      "review: 0.319762647151947\n",
      "robotics: 0.19871310889720917\n",
      "salary: 0.40827029943466187\n",
      "simulation: 0.285733699798584\n",
      "software: 0.48992857336997986\n",
      "sql: 0.3428829610347748\n",
      "statistics: 0.32742029428482056\n",
      "system design: 0.4432872235774994\n",
      "theory: 0.3361312448978424\n",
      "tools: 0.3426421880722046\n",
      "trading: 0.42205727100372314\n",
      "tutorial: 0.33039993047714233\n",
      "tutorials: 0.34844866394996643\n",
      "use case: 0.4509735703468323\n",
      "use cases: 0.26727837324142456\n",
      "ux: 0.5833836793899536\n",
      "visualization: 0.3853481411933899\n",
      "web development: 0.5372061133384705\n"
     ]
    }
   ],
   "source": [
    "_, test_prediction = model(encoding[\"input_ids\"], encoding[\"attention_mask\"])\n",
    "test_prediction = test_prediction.flatten().numpy()\n",
    "\n",
    "for label, prediction in zip(LABEL_COLUMNS, test_prediction):\n",
    "  print(f\"{label}: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering: 0.5440770983695984\n",
      "computer vision: 0.5074830055236816\n",
      "courses: 0.5072101354598999\n",
      "healthcare: 0.5016406178474426\n",
      "iot: 0.5198387503623962\n",
      "nlp: 0.6035922765731812\n",
      "ux: 0.5572955012321472\n",
      "web development: 0.5504329800605774\n"
     ]
    }
   ],
   "source": [
    "THRESHOLD = 0.5\n",
    "\n",
    "test_comment = \"Big Data at Air Miles\"\n",
    "encoding = tokenizer.encode_plus(\n",
    "  test_comment,\n",
    "  add_special_tokens=True,\n",
    "  max_length=512,\n",
    "  return_token_type_ids=False,\n",
    "  padding=\"max_length\",\n",
    "  return_attention_mask=True,\n",
    "  return_tensors='pt',\n",
    ")\n",
    "\n",
    "_, test_prediction = model(encoding[\"input_ids\"], encoding[\"attention_mask\"])\n",
    "test_prediction = test_prediction.flatten().numpy()\n",
    "\n",
    "for label, prediction in zip(LABEL_COLUMNS, test_prediction):\n",
    "  if prediction < THRESHOLD:\n",
    "    continue\n",
    "  print(f\"{label}: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKEN_COUNT = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:35<00:00,  1.31it/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "trained_model = model.to(device)\n",
    "\n",
    "val_dataset = PostTitleDataset(\n",
    "  val,\n",
    "  tokenizer,\n",
    "  max_token_len=MAX_TOKEN_COUNT\n",
    ")\n",
    "\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "for item in tqdm(val_dataset):\n",
    "  _, prediction = trained_model(\n",
    "    item[\"input_ids\"].unsqueeze(dim=0).to(device), \n",
    "    item[\"attention_mask\"].unsqueeze(dim=0).to(device)\n",
    "  )\n",
    "  predictions.append(prediction.flatten())\n",
    "  labels.append(item[\"labels\"].int())\n",
    "\n",
    "predictions = torch.stack(predictions).detach().cpu()\n",
    "labels = torch.stack(labels).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8801)"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(predictions, labels, threshold=THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC per tag\n",
      "ai: 0.8222222328186035\n",
      "algorithm: 0.5555555820465088\n",
      "algorithms: 0.6022727489471436\n",
      "application: 0.7596899271011353\n",
      "applications: 0.0\n",
      "bandits: 0.0\n",
      "bayesian: 0.0\n",
      "best practice: 0.0\n",
      "best practices: 0.0\n",
      "big data: 0.4545454680919647\n",
      "book: 0.0\n",
      "books: 0.0\n",
      "business: 0.0\n",
      "career: 0.19999998807907104\n",
      "cloud: 0.0\n",
      "clustering: 0.0\n",
      "computer science: 0.511904776096344\n",
      "computer vision: 0.738636314868927\n",
      "course: 0.5227272510528564\n",
      "courses: 0.0\n",
      "dashboard: 0.0\n",
      "data: 0.2888888716697693\n",
      "data engineering: 0.0\n",
      "data mining: 0.6666666269302368\n",
      "data science: 0.27906978130340576\n",
      "data shift: 0.0\n",
      "data visualization: 0.0\n",
      "dataset: 0.0\n",
      "decision making: 0.9545454382896423\n",
      "deep learning: 0.4728682041168213\n",
      "design patterns: 0.0\n",
      "designed patterns: 0.0\n",
      "engineering: 0.0\n",
      "experimentation: 0.0\n",
      "forecasting: 0.7111110687255859\n",
      "gis: 0.1111111044883728\n",
      "graph: 0.35555553436279297\n",
      "guide: 0.5333333015441895\n",
      "hardware: 0.0\n",
      "healthcare: 0.0\n",
      "hiring: 0.0\n",
      "how-to: 0.0\n",
      "improvement: 0.8888888359069824\n",
      "industry: 0.6222221851348877\n",
      "infrastructure: 0.0\n",
      "interview: 0.644444465637207\n",
      "interviews: 0.0\n",
      "iot: 0.0\n",
      "libraries: 0.0\n",
      "library: 0.0\n",
      "machine learning: 0.545787513256073\n",
      "mathematics: 0.2634146213531494\n",
      "mlops: 0.5454545021057129\n",
      "mobile development: 0.0\n",
      "neural network: 0.0\n",
      "nlp: 0.6000000238418579\n",
      "notes: 0.0\n",
      "paper: 0.7333333492279053\n",
      "papers: 0.0\n",
      "predictive analytics: 0.0\n",
      "probability: 0.35555553436279297\n",
      "product: 0.0\n",
      "programming: 0.0\n",
      "python: 0.6666666269302368\n",
      "question: 0.0\n",
      "rant: 0.0\n",
      "real-time: 0.0\n",
      "recommendation system: 0.7777777910232544\n",
      "reinforcement learning: 0.0\n",
      "research: 0.4444444179534912\n",
      "resource: 0.0\n",
      "review: 0.0\n",
      "robotics: 0.0\n",
      "salary: 0.0\n",
      "simulation: 0.0\n",
      "software: 0.0\n",
      "sql: 0.0\n",
      "statistics: 0.9545454382896423\n",
      "system design: 0.0\n",
      "theory: 0.0\n",
      "tools: 0.0\n",
      "trading: 0.0\n",
      "tutorial: 0.0\n",
      "tutorials: 0.0\n",
      "use case: 0.02222222089767456\n",
      "use cases: 0.0\n",
      "ux: 0.0\n",
      "visualization: 0.7045454382896423\n",
      "web development: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/schen/.pyenv/versions/3.7.8/envs/wecloud378/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "for i, name in enumerate(LABEL_COLUMNS):\n",
    "  tag_auroc = auroc(predictions[:, i], labels[:, i], pos_label=1)\n",
    "  print(f\"{name}: {tag_auroc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "                    ai       0.04      1.00      0.07         1\n",
      "             algorithm       0.00      0.00      0.00         1\n",
      "            algorithms       0.00      0.00      0.00         2\n",
      "           application       0.00      0.00      0.00         3\n",
      "          applications       0.00      0.00      0.00         0\n",
      "               bandits       0.00      0.00      0.00         0\n",
      "              bayesian       0.00      0.00      0.00         0\n",
      "         best practice       0.00      0.00      0.00         0\n",
      "        best practices       0.00      0.00      0.00         0\n",
      "              big data       0.00      0.00      0.00         2\n",
      "                  book       0.00      0.00      0.00         0\n",
      "                 books       0.00      0.00      0.00         0\n",
      "              business       0.00      0.00      0.00         0\n",
      "                career       0.02      1.00      0.04         1\n",
      "                 cloud       0.00      0.00      0.00         0\n",
      "            clustering       0.00      0.00      0.00         0\n",
      "      computer science       0.00      0.00      0.00         4\n",
      "       computer vision       0.00      0.00      0.00         2\n",
      "                course       0.00      0.00      0.00         2\n",
      "               courses       0.00      0.00      0.00         0\n",
      "             dashboard       0.00      0.00      0.00         0\n",
      "                  data       0.00      0.00      0.00         1\n",
      "      data engineering       0.00      0.00      0.00         0\n",
      "           data mining       0.00      0.00      0.00         1\n",
      "          data science       0.00      0.00      0.00         3\n",
      "            data shift       0.00      0.00      0.00         0\n",
      "    data visualization       0.00      0.00      0.00         0\n",
      "               dataset       0.00      0.00      0.00         0\n",
      "       decision making       0.00      0.00      0.00         2\n",
      "         deep learning       0.00      0.00      0.00         3\n",
      "       design patterns       0.00      0.00      0.00         0\n",
      "     designed patterns       0.00      0.00      0.00         0\n",
      "           engineering       0.00      0.00      0.00         0\n",
      "       experimentation       0.00      0.00      0.00         0\n",
      "           forecasting       0.00      0.00      0.00         1\n",
      "                   gis       0.00      0.00      0.00         1\n",
      "                 graph       0.00      0.00      0.00         1\n",
      "                 guide       0.00      0.00      0.00         1\n",
      "              hardware       0.00      0.00      0.00         0\n",
      "            healthcare       0.00      0.00      0.00         0\n",
      "                hiring       0.00      0.00      0.00         0\n",
      "                how-to       0.00      0.00      0.00         0\n",
      "           improvement       0.00      0.00      0.00         1\n",
      "              industry       0.00      0.00      0.00         1\n",
      "        infrastructure       0.00      0.00      0.00         0\n",
      "             interview       0.00      0.00      0.00         1\n",
      "            interviews       0.00      0.00      0.00         0\n",
      "                   iot       0.00      0.00      0.00         0\n",
      "             libraries       0.00      0.00      0.00         0\n",
      "               library       0.00      0.00      0.00         0\n",
      "      machine learning       0.15      1.00      0.26         7\n",
      "           mathematics       0.00      0.00      0.00         5\n",
      "                 mlops       0.00      0.00      0.00         2\n",
      "    mobile development       0.00      0.00      0.00         0\n",
      "        neural network       0.00      0.00      0.00         0\n",
      "                   nlp       0.00      0.00      0.00         1\n",
      "                 notes       0.00      0.00      0.00         0\n",
      "                 paper       0.00      0.00      0.00         1\n",
      "                papers       0.00      0.00      0.00         0\n",
      "  predictive analytics       0.00      0.00      0.00         0\n",
      "           probability       0.00      0.00      0.00         1\n",
      "               product       0.00      0.00      0.00         0\n",
      "           programming       0.00      0.00      0.00         0\n",
      "                python       0.00      0.00      0.00         1\n",
      "              question       0.00      0.00      0.00         0\n",
      "                  rant       0.00      0.00      0.00         0\n",
      "             real-time       0.00      0.00      0.00         0\n",
      " recommendation system       0.00      0.00      0.00         1\n",
      "reinforcement learning       0.00      0.00      0.00         0\n",
      "              research       0.00      0.00      0.00         1\n",
      "              resource       0.00      0.00      0.00         0\n",
      "                review       0.00      0.00      0.00         0\n",
      "              robotics       0.00      0.00      0.00         0\n",
      "                salary       0.00      0.00      0.00         0\n",
      "            simulation       0.00      0.00      0.00         0\n",
      "              software       0.00      0.00      0.00         0\n",
      "                   sql       0.00      0.00      0.00         0\n",
      "            statistics       0.00      0.00      0.00         2\n",
      "         system design       0.00      0.00      0.00         0\n",
      "                theory       0.00      0.00      0.00         0\n",
      "                 tools       0.00      0.00      0.00         0\n",
      "               trading       0.00      0.00      0.00         0\n",
      "              tutorial       0.00      0.00      0.00         0\n",
      "             tutorials       0.00      0.00      0.00         0\n",
      "              use case       0.02      1.00      0.04         1\n",
      "             use cases       0.00      0.00      0.00         0\n",
      "                    ux       0.00      0.00      0.00         0\n",
      "         visualization       0.00      0.00      0.00         2\n",
      "       web development       0.00      0.00      0.00         0\n",
      "\n",
      "             micro avg       0.02      0.17      0.04        60\n",
      "             macro avg       0.00      0.04      0.00        60\n",
      "          weighted avg       0.02      0.17      0.03        60\n",
      "           samples avg       0.02      0.14      0.04        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = predictions.numpy()\n",
    "y_true = labels.numpy()\n",
    "\n",
    "upper, lower = 1, 0\n",
    "\n",
    "y_pred = np.where(y_pred > THRESHOLD, upper, lower)\n",
    "\n",
    "print(classification_report(\n",
    "  y_true, \n",
    "  y_pred, \n",
    "  target_names=LABEL_COLUMNS, \n",
    "  zero_division=0\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit ('wecloud378')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f9e2d74152e24ab400ff90e1d5e966b64bc5f8804979a3012a2aff06a2de9e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
